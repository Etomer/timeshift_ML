{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import glob\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from bisect import bisect_left\n",
    "import scipy as sp\n",
    "import h5py\n",
    "from scipy.io import wavfile\n",
    "from glob import glob\n",
    "\n",
    "import wandb\n",
    "\n",
    "data_folder = \"./datasets/generated_dataset/\"\n",
    "device = 'cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dropout\" : 0.1,\n",
    "    \"batch_size\" : 2, \n",
    "    \"max_shift\" : 500,\n",
    "    \"guess_grid_size\" : 100,\n",
    "    \"dataset\" : \"new_type_dataset_medium.hdf5\",\n",
    "    \"cnn_output_size_at_factor_1\" : 576,\n",
    "    \"factor\" : 10,\n",
    "    \"loss_fn\" : \"cross_entropy\",\n",
    "    \"epochs\" : 200,\n",
    "    \"sample_length\" : 10000,\n",
    "    \"max_shift\" : 100,\n",
    "    \"lr\" : 1e-6,\n",
    "    \"n_batch_before_print\" : 1,\n",
    "    \"max_freq\" : 2500,\n",
    "    \"rir_len\" : 1600,\n",
    "    \"rooms_per_batch\" : 2,\n",
    "    \"mics_per_batch\" : 17,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"../datasets/generated_dataset/\" + config[\"dataset\"],\"r\")\n",
    "    #f = h5py.File(\"./datasets/generated_dataset/generated_dataset_medium.hdf5\",\"r\")\n",
    "\n",
    "X = f['input']\n",
    "y = f['gt']\n",
    "\n",
    "# f_test = h5py.File(\"../datasets/generated_dataset/generated_dataset_hard.hdf5\",\"r\")\n",
    "# X_test = f_test['input']\n",
    "# y_test = f_test['gt']\n",
    "\n",
    "#X = torch.load(os.path.join(data_folder,\"input2.pt\"))\n",
    "#y = torch.load(os.path.join(data_folder,\"gt2.pt\"))\n",
    "#X_test = torch.load(os.path.join(data_folder,\"input.pt\"))\n",
    "#y_test = torch.load(os.path.join(data_folder,\"gt.pt\"))\n",
    "\n",
    "class custom_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y, idx_min=0,dataset_len=len(X)):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.dataset_len = dataset_len\n",
    "        self.idx_min = idx_min\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx + self.idx_min],self.y[idx + self.idx_min]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "\n",
    "split_i = int(X.shape[0]*0.9)\n",
    "dataset = custom_dataset(X,y, 0, split_i)\n",
    "#dataset_test = custom_dataset(X_test,y_test, 0 ,X_test.shape[0])\n",
    "dataset_test = custom_dataset(X,y, split_i, X.shape[0] - split_i)\n",
    "#dataset, dataset_test = torch.utils.data.random_split(dataset, [0.98,0.02])\n",
    "train_dl = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "test_dl = DataLoader(dataset_test, batch_size=config[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,size):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "        self.l = nn.Linear(size,size)\n",
    "        self.l2 = nn.Linear(size,size)\n",
    "        self.act = nn.GELU()\n",
    "        #self.ln = nn.LayerNorm(size)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.002)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.002)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        #return x + self.act(self.l(self.ln(x)))\n",
    "        return x + self.l2(self.act(self.l(x)))\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.thinker = nn.Sequential(\n",
    "            #nn.Dropout(dropout),\n",
    "            Block(10000),\n",
    "            nn.Linear(10000,config[\"guess_grid_size\"])\n",
    "            #nn.Dropout(dropout),\n",
    "            #Block(cnn_output_size_at_factor_1*factor),\n",
    "            #nn.Dropout(dropout),\n",
    "            #Block(cnn_output_size_at_factor_1*factor),\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.002)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.002)\n",
    "        \n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        x = self.thinker(x)\n",
    "        return x\n",
    "    \n",
    "model = Classifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"loss_fn\"] == \"cross_entropy\":\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def y_to_class_gt(y, guess_grid_size, max_shift) :\n",
    "    y[y.abs() > max_shift] = max_shift*y[y.abs() > max_shift].sign()\n",
    "\n",
    "    bin_width = max_shift*2/guess_grid_size\n",
    "    y = (y/bin_width).round() + guess_grid_size // 2 \n",
    "    y[y == guess_grid_size] = guess_grid_size - 1\n",
    "    return y.long()\n",
    "    \n",
    "\n",
    "#loss_fn = torch.nn.HuberLoss(delta=10)\n",
    "def augment_switch(X,y):\n",
    "    flips = torch.rand(y.shape[0]) < 0.5\n",
    "    X[flips] = torch.stack([X[flips,:,:,1], X[flips,:,:,0]], dim=3)\n",
    "    y[flips] = -y[flips]\n",
    "    return X,y\n",
    "\n",
    "def augment_shift(X,y, common_shift=True):\n",
    "    X = torch.complex(X[:,0],X[:,1])\n",
    "\n",
    "    #augment 1, multiply each of the vectors with phase\n",
    "\n",
    "    imag_unit = torch.complex(torch.tensor(0.0),torch.tensor(1.0))\n",
    "    if common_shift:\n",
    "        sample_shift = (torch.rand(X.shape[0],1,1)*2 - 1)*config[\"max_shift\"]\n",
    "        phase_shift = (-sample_shift/config[\"sample_length\"]*2*torch.pi*imag_unit*torch.arange(X.shape[1]).unsqueeze(1)).exp()\n",
    "        new_abf = X*phase_shift\n",
    "        X = torch.stack([torch.real(new_abf),torch.imag(new_abf)], dim=1)\n",
    "        \n",
    "    else:\n",
    "        sample_shift = (torch.rand(X.shape[0],1,2)*2 - 1)*config[\"max_shift\"]\n",
    "        phase_shift = (-sample_shift/config[\"sample_length\"]*2*torch.pi*imag_unit*torch.arange(X.shape[1]).unsqueeze(1)).exp()\n",
    "        new_abf = X*phase_shift\n",
    "        X = torch.stack([torch.real(new_abf),torch.imag(new_abf)], dim=1)\n",
    "        y = y + (sample_shift[:,:,0] - sample_shift[:,:,1]).to(int)\n",
    "    return (X,y)\n",
    "\n",
    "def augment_amp(X,y):\n",
    "    amp_max = 3\n",
    "    amp_min = 0.2\n",
    "    amp_change_factor = torch.rand(X.shape)*(amp_max - amp_min) + amp_min\n",
    "    X = X*amp_change_factor\n",
    "    return (X,y)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    save_loss_to_file = []\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    print_loss = 0\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        \n",
    "        # Compute prediction error\n",
    "        \n",
    "        \n",
    "        X = torch.tensor(X).to(device)\n",
    "\n",
    "        #pull a random sound\n",
    "        sound_paths = glob(\"../datasets/reference_sounds/*.wav\")\n",
    "        sound_path = sound_paths[np.random.randint(len(sound_paths))]\n",
    "        fs,signal = wavfile.read(sound_path)\n",
    "        start = np.random.randint(0,len(signal) - config[\"sample_length\"] - config[\"rir_len\"]-1, config[\"rooms_per_batch\"])\n",
    "        # simulate longer sound and then cut to the relevant piece\n",
    "        signals = np.zeros((config[\"rooms_per_batch\"], config[\"sample_length\"] + config[\"rir_len\"]-1))\n",
    "        for i in range(config[\"rooms_per_batch\"]):\n",
    "            signals[i,:] = signal[start[i]:start[i]  + config[\"sample_length\"] + config[\"rir_len\"]-1]\n",
    "        signals = torch.tensor(signals).to(torch.float32).to(device).unsqueeze(1)\n",
    "\n",
    "\n",
    "        #plt.plot(np.convolve(X[0],signal))\n",
    "        #plt.plot(signal + 2*max(signal))\n",
    "\n",
    "        #torch.nn.functional.conv1d(signal.unsqueeze(1),X.unsqueeze(1))\n",
    "        #ex1 = torch.fft.rfft(torch.concatenate([torch.zeros(rir_len),torch.ones(sample_length),torch.zeros(rir_len)]))*torch.fft.rfft(X,signal.shape[1]) * torch.fft.rfft(signal)\n",
    "\n",
    "        q = torch.fft.rfft(torch.fft.irfft(torch.fft.rfft(signals)*torch.fft.rfft(X,signals.shape[1]))[:,:,:config[\"sample_length\"]])[:,:,:config[\"max_freq\"]]\n",
    "        q = torch.concatenate([torch.concatenate([q,q.roll(i+1, 1)], dim=2) for i in range(config[\"mics_per_batch\"] // 2)],dim=1)\n",
    "        q = q.view(config[\"rooms_per_batch\"]*(config[\"mics_per_batch\"]*(config[\"mics_per_batch\"] - 1 ))//2, -1)\n",
    "        q = torch.concatenate([q.real,q.imag],dim=1)\n",
    "        y = torch.concatenate([y - y.roll(i+1,1) for i in range(config[\"mics_per_batch\"]//2)],dim=1).view(-1)*fs/343\n",
    "        y = y_to_class_gt(y, config[\"guess_grid_size\"], config[\"max_shift\"]).to(torch.long)\n",
    "        y = torch.tensor(y).to(device)\n",
    "        pred = model(q)\n",
    "        \n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print_loss += loss.detach()/config[\"n_batch_before_print\"]\n",
    "        if batch % config[\"n_batch_before_print\"] == 0:\n",
    "            if batch == 0:\n",
    "                print_loss = 0 \n",
    "                continue\n",
    "            #wandb.log({\"loss\":loss})\n",
    "            loss, current = print_loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            print_loss = 0\n",
    "            save_loss_to_file.append((loss, current))\n",
    "    return save_loss_to_file\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    model.eval() # regression on dropout is not great\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #model.train()\n",
    "        print_loss = 0\n",
    "        counter = 0\n",
    "        for batch, (X,y) in enumerate(dataloader):\n",
    "            \n",
    "            # Compute prediction error\n",
    "            X = torch.concatenate([X[:,:,:,0], X[:,:,:,1]], dim=1)\n",
    "            X /= 1e6\n",
    "            X = X.to(device)\n",
    "            y = y_to_class_gt(y.squeeze(1), config[\"guess_grid_size\"], config[\"max_shift\"])\n",
    "            #y = y.to(int).squeeze(1) + guess_grid_size // 2\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            \n",
    "            \n",
    "\n",
    "            loss = loss_fn(pred, y)\n",
    "                \n",
    "            print_loss += loss.detach()\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "        loss = print_loss.item()/counter\n",
    "        wandb.log({\"Test_loss\":loss})\n",
    "        print(f\"Test loss: {loss:>7f}\")\n",
    "        print_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_200480/1732332010.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X).to(device)\n",
      "/tmp/ipykernel_200480/1732332010.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 236.392990  [    2/   18]\n",
      "loss: 40.073994  [    4/   18]\n",
      "loss: 133.630234  [    6/   18]\n",
      "loss: 16.791235  [    8/   18]\n",
      "loss: 123.176109  [   10/   18]\n",
      "loss: 51.577168  [   12/   18]\n",
      "loss: 219.341675  [   14/   18]\n",
      "loss: 232.183411  [   16/   18]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 66.078537  [    2/   18]\n",
      "loss: 190.903259  [    4/   18]\n",
      "loss: 540.439270  [    6/   18]\n",
      "loss: 44.570305  [    8/   18]\n",
      "loss: 92.740784  [   10/   18]\n",
      "loss: 17.559750  [   12/   18]\n",
      "loss: 92.443390  [   14/   18]\n",
      "loss: 31.948336  [   16/   18]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 423.491943  [    2/   18]\n",
      "loss: 57.314255  [    4/   18]\n",
      "loss: 39.926334  [    6/   18]\n",
      "loss: 63.145840  [    8/   18]\n",
      "loss: 419.816559  [   10/   18]\n",
      "loss: 126.815186  [   12/   18]\n",
      "loss: 88.057343  [   14/   18]\n",
      "loss: 59.285797  [   16/   18]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 22.005112  [    2/   18]\n",
      "loss: 337.081360  [    4/   18]\n",
      "loss: 235.015686  [    6/   18]\n",
      "loss: 35.253273  [    8/   18]\n",
      "loss: 80.389603  [   10/   18]\n",
      "loss: 169.393799  [   12/   18]\n",
      "loss: 48.173870  [   14/   18]\n",
      "loss: 46.508652  [   16/   18]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 203.753922  [    2/   18]\n",
      "loss: 36.858509  [    4/   18]\n",
      "loss: 74.458710  [    6/   18]\n",
      "loss: 162.101715  [    8/   18]\n",
      "loss: 136.991226  [   10/   18]\n",
      "loss: 214.288605  [   12/   18]\n",
      "loss: 31.161905  [   14/   18]\n",
      "loss: 320.311920  [   16/   18]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 133.794113  [    2/   18]\n",
      "loss: 60.276936  [    4/   18]\n",
      "loss: 54.139496  [    6/   18]\n",
      "loss: 703.158386  [    8/   18]\n",
      "loss: 279.784576  [   10/   18]\n",
      "loss: 193.903732  [   12/   18]\n",
      "loss: 35.245316  [   14/   18]\n",
      "loss: 108.271713  [   16/   18]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 112.021103  [    2/   18]\n",
      "loss: 30.973532  [    4/   18]\n",
      "loss: 86.900696  [    6/   18]\n",
      "loss: 302.980530  [    8/   18]\n",
      "loss: 54.937950  [   10/   18]\n",
      "loss: 113.447189  [   12/   18]\n",
      "loss: 131.146393  [   14/   18]\n",
      "loss: 528.842041  [   16/   18]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 160.364456  [    2/   18]\n",
      "loss: 229.279526  [    4/   18]\n",
      "loss: 51.832001  [    6/   18]\n",
      "loss: 74.583191  [    8/   18]\n",
      "loss: 128.141937  [   10/   18]\n",
      "loss: 167.360031  [   12/   18]\n",
      "loss: 448.372314  [   14/   18]\n",
      "loss: 14.889007  [   16/   18]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 29.102533  [    2/   18]\n",
      "loss: 306.554169  [    4/   18]\n",
      "loss: 331.403656  [    6/   18]\n",
      "loss: 96.549164  [    8/   18]\n",
      "loss: 91.961876  [   10/   18]\n",
      "loss: 114.323975  [   12/   18]\n",
      "loss: 215.713806  [   14/   18]\n",
      "loss: 123.091370  [   16/   18]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 101.172760  [    2/   18]\n",
      "loss: 118.903137  [    4/   18]\n",
      "loss: 128.966385  [    6/   18]\n",
      "loss: 265.499756  [    8/   18]\n",
      "loss: 123.483582  [   10/   18]\n",
      "loss: 154.353058  [   12/   18]\n",
      "loss: 145.000565  [   14/   18]\n",
      "loss: 330.128815  [   16/   18]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 114.607887  [    2/   18]\n",
      "loss: 42.460121  [    4/   18]\n",
      "loss: 77.061127  [    6/   18]\n",
      "loss: 9.072883  [    8/   18]\n",
      "loss: 421.303772  [   10/   18]\n",
      "loss: 649.996765  [   12/   18]\n",
      "loss: 107.450211  [   14/   18]\n",
      "loss: 26.782532  [   16/   18]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 55.336941  [    2/   18]\n",
      "loss: 211.568802  [    4/   18]\n",
      "loss: 313.949493  [    6/   18]\n",
      "loss: 22.790268  [    8/   18]\n",
      "loss: 249.696381  [   10/   18]\n",
      "loss: 93.857674  [   12/   18]\n",
      "loss: 59.510727  [   14/   18]\n",
      "loss: 498.691650  [   16/   18]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 79.674019  [    2/   18]\n",
      "loss: 75.827698  [    4/   18]\n",
      "loss: 154.474548  [    6/   18]\n",
      "loss: 247.343140  [    8/   18]\n",
      "loss: 164.573059  [   10/   18]\n",
      "loss: 49.941048  [   12/   18]\n",
      "loss: 173.618454  [   14/   18]\n",
      "loss: 10.907428  [   16/   18]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 22.030615  [    2/   18]\n",
      "loss: 165.830353  [    4/   18]\n",
      "loss: 122.291924  [    6/   18]\n",
      "loss: 960.906494  [    8/   18]\n",
      "loss: 99.959778  [   10/   18]\n",
      "loss: 31.628435  [   12/   18]\n",
      "loss: 52.241425  [   14/   18]\n",
      "loss: 128.249603  [   16/   18]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 51.969227  [    2/   18]\n",
      "loss: 211.880127  [    4/   18]\n",
      "loss: 38.814304  [    6/   18]\n",
      "loss: 77.083328  [    8/   18]\n",
      "loss: 83.325340  [   10/   18]\n",
      "loss: 33.090530  [   12/   18]\n",
      "loss: 93.583817  [   14/   18]\n",
      "loss: 764.847412  [   16/   18]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 110.829117  [    2/   18]\n",
      "loss: 43.825733  [    4/   18]\n",
      "loss: 96.124298  [    6/   18]\n",
      "loss: 45.896885  [    8/   18]\n",
      "loss: 156.809830  [   10/   18]\n",
      "loss: 337.043762  [   12/   18]\n",
      "loss: 882.918457  [   14/   18]\n",
      "loss: 37.029911  [   16/   18]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 85.267204  [    2/   18]\n",
      "loss: 342.947662  [    4/   18]\n",
      "loss: 325.577393  [    6/   18]\n",
      "loss: 85.555809  [    8/   18]\n",
      "loss: 56.554901  [   10/   18]\n",
      "loss: 72.825142  [   12/   18]\n",
      "loss: 229.326691  [   14/   18]\n",
      "loss: 339.061340  [   16/   18]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 66.221634  [    2/   18]\n",
      "loss: 178.242737  [    4/   18]\n",
      "loss: 111.059753  [    6/   18]\n",
      "loss: 26.274982  [    8/   18]\n",
      "loss: 85.405983  [   10/   18]\n",
      "loss: 122.442856  [   12/   18]\n",
      "loss: 72.269890  [   14/   18]\n",
      "loss: 155.895782  [   16/   18]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 520.946106  [    2/   18]\n",
      "loss: 163.107361  [    4/   18]\n",
      "loss: 62.445850  [    6/   18]\n",
      "loss: 73.199760  [    8/   18]\n",
      "loss: 153.102890  [   10/   18]\n",
      "loss: 52.932835  [   12/   18]\n",
      "loss: 138.542465  [   14/   18]\n",
      "loss: 16.489027  [   16/   18]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 63.439438  [    2/   18]\n",
      "loss: 97.349220  [    4/   18]\n",
      "loss: 89.215759  [    6/   18]\n",
      "loss: 453.027924  [    8/   18]\n",
      "loss: 45.407940  [   10/   18]\n",
      "loss: 211.591263  [   12/   18]\n",
      "loss: 62.644123  [   14/   18]\n",
      "loss: 62.577850  [   16/   18]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 78.836098  [    2/   18]\n",
      "loss: 216.214172  [    4/   18]\n",
      "loss: 20.182735  [    6/   18]\n",
      "loss: 119.045738  [    8/   18]\n",
      "loss: 50.153931  [   10/   18]\n",
      "loss: 120.901772  [   12/   18]\n",
      "loss: 55.030396  [   14/   18]\n",
      "loss: 64.673012  [   16/   18]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 53.943737  [    2/   18]\n",
      "loss: 222.366226  [    4/   18]\n",
      "loss: 187.083954  [    6/   18]\n",
      "loss: 268.639648  [    8/   18]\n",
      "loss: 292.945709  [   10/   18]\n",
      "loss: 88.308846  [   12/   18]\n",
      "loss: 53.263947  [   14/   18]\n",
      "loss: 254.467896  [   16/   18]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 10.260827  [    2/   18]\n",
      "loss: 456.963623  [    4/   18]\n",
      "loss: 253.678650  [    6/   18]\n",
      "loss: 98.890663  [    8/   18]\n",
      "loss: 209.330826  [   10/   18]\n",
      "loss: 26.031603  [   12/   18]\n",
      "loss: 12.160542  [   14/   18]\n",
      "loss: 481.772339  [   16/   18]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 64.817528  [    2/   18]\n",
      "loss: 111.594917  [    4/   18]\n",
      "loss: 172.041138  [    6/   18]\n",
      "loss: 8.501224  [    8/   18]\n",
      "loss: 567.600098  [   10/   18]\n",
      "loss: 81.554184  [   12/   18]\n",
      "loss: 273.995880  [   14/   18]\n",
      "loss: 559.213379  [   16/   18]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 153.094177  [    2/   18]\n",
      "loss: 150.216537  [    4/   18]\n",
      "loss: 65.572693  [    6/   18]\n",
      "loss: 70.991898  [    8/   18]\n",
      "loss: 18.297091  [   10/   18]\n",
      "loss: 258.512665  [   12/   18]\n",
      "loss: 82.001198  [   14/   18]\n",
      "loss: 35.468479  [   16/   18]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 93.117287  [    2/   18]\n",
      "loss: 58.383648  [    4/   18]\n",
      "loss: 27.745344  [    6/   18]\n",
      "loss: 196.304291  [    8/   18]\n",
      "loss: 136.625748  [   10/   18]\n",
      "loss: 135.338287  [   12/   18]\n",
      "loss: 223.087967  [   14/   18]\n",
      "loss: 140.346115  [   16/   18]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 79.808548  [    2/   18]\n",
      "loss: 72.083954  [    4/   18]\n",
      "loss: 115.405968  [    6/   18]\n",
      "loss: 81.053818  [    8/   18]\n",
      "loss: 15.516968  [   10/   18]\n",
      "loss: 72.877678  [   12/   18]\n",
      "loss: 495.692719  [   14/   18]\n",
      "loss: 93.222282  [   16/   18]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 169.614868  [    2/   18]\n",
      "loss: 182.054657  [    4/   18]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m#torch.save(model, \"model_musan_epoch_\" + str(t)+ \".pth\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m#loss_to_file = test(test_dl, model, loss_fn)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     loss_to_file \u001b[39m=\u001b[39m train(train_dl, model, loss_fn, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m#losses = np.concatenate([losses,np.array(loss_to_file\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# )])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m#np.save(\"losses\", losses)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#torch.save(model, \"./models/\" + config[\"dataset\"].split(\".\")[0]+ \"_\" + run.name + \".pth\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39m#wandb.log({\"loss\":loss})\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m loss, current \u001b[39m=\u001b[39m print_loss\u001b[39m.\u001b[39;49mitem(), batch \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(X)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m>7f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m  [\u001b[39m\u001b[39m{\u001b[39;00mcurrent\u001b[39m:\u001b[39;00m\u001b[39m>5d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m:\u001b[39;00m\u001b[39m>5d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bnewton.maths.lu.se/home2/er6236te/timeshift_ML/scripts/training_on_new_generated_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m print_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(config[\"epochs\"]):\n",
    "    losses = np.zeros((0,2))\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    #torch.save(model, \"model_musan_epoch_\" + str(t)+ \".pth\")\n",
    "\n",
    "    #loss_to_file = test(test_dl, model, loss_fn)\n",
    "    loss_to_file = train(train_dl, model, loss_fn, optimizer)\n",
    "\n",
    "    \n",
    "    #losses = np.concatenate([losses,np.array(loss_to_file\n",
    "    # )])\n",
    "    #np.save(\"losses\", losses)\n",
    "\n",
    "#torch.save(model, \"./models/\" + config[\"dataset\"].split(\".\")[0]+ \"_\" + run.name + \".pth\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
