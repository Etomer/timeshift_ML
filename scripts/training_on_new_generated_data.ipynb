{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (5.2.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import glob\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from bisect import bisect_left\n",
    "import scipy as sp\n",
    "import h5py\n",
    "from scipy.io import wavfile\n",
    "from glob import glob\n",
    "\n",
    "import wandb\n",
    "\n",
    "use_wandb = False\n",
    "\n",
    "data_folder = \"./datasets/generated_dataset/\"\n",
    "device = 'cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dropout\" : 0.1,\n",
    "    \"batch_size\" : 50, \n",
    "    \"max_shift\" : 500, # ~10 meters\n",
    "    \"guess_grid_size\" : 100,\n",
    "    \"dataset\" : \"new_type_dataset_medium.hdf5\",\n",
    "    \"cnn_output_size_at_factor_1\" : 576,\n",
    "    \"factor\" : 10,\n",
    "    \"loss_fn\" : \"cross_entropy\",\n",
    "    \"epochs\" : 200,\n",
    "    \"sample_length\" : 10000,\n",
    "    \"max_shift\" : 100,\n",
    "    \"lr\" : 1e-4,\n",
    "    \"n_batch_before_print\" : 1,\n",
    "    \"max_freq\" : 2500,\n",
    "    \"rir_len\" : 1600,\n",
    "    \"rooms_per_batch\" : 50,\n",
    "    \"mics_per_batch\" : 11,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"../datasets/generated_dataset/\" + config[\"dataset\"],\"r\")\n",
    "\n",
    "X = f['input']\n",
    "y = f['gt']\n",
    "\n",
    "class custom_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y, idx_min=0,dataset_len=len(X)):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.dataset_len = dataset_len\n",
    "        self.idx_min = idx_min\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx + self.idx_min,:config[\"mics_per_batch\"]],self.y[idx + self.idx_min,:config[\"mics_per_batch\"]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "\n",
    "# package datasets\n",
    "split_i = int(X.shape[0]*0.9)\n",
    "dataset = custom_dataset(X,y, 0, split_i)\n",
    "dataset_test = custom_dataset(X,y, split_i, X.shape[0] - split_i)\n",
    "train_dl = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "test_dl = DataLoader(dataset_test, batch_size=config[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,size):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "        self.l = nn.Linear(size,2*size)\n",
    "        self.l2 = nn.Linear(2*size,size)\n",
    "        self.act = nn.GELU()\n",
    "        self.ln = nn.LayerNorm(size)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.002)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.002)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return x + self.l2(self.act(self.l(self.ln(self.dropout(x)))))\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.thinker = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(config[\"dropout\"]),\n",
    "            nn.Linear(config[\"factor\"]*config[\"cnn_output_size_at_factor_1\"],1000),\n",
    "            nn.GELU(),\n",
    "            Block(1000),\n",
    "            Block(1000),\n",
    "            nn.Linear(1000,config[\"guess_grid_size\"])\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(4,48*config[\"factor\"], 50,stride=5),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(48*config[\"factor\"],48*config[\"factor\"], 50,stride=5),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(48*config[\"factor\"],48*config[\"factor\"], 30,stride=5),\n",
    "            nn.GELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.0002)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.0002)\n",
    "        \n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.thinker(x)\n",
    "        return x\n",
    "    \n",
    "model = Classifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"loss_fn\"] == \"cross_entropy\":\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def y_to_class_gt(y, guess_grid_size, max_shift) :\n",
    "    y[y.abs() > max_shift] = max_shift*y[y.abs() > max_shift].sign()\n",
    "\n",
    "    bin_width = max_shift*2/guess_grid_size\n",
    "    y = (y/bin_width).round() + guess_grid_size // 2 \n",
    "    y[y == guess_grid_size] = guess_grid_size - 1\n",
    "    return y.long()\n",
    "    \n",
    "\n",
    "# #loss_fn = torch.nn.HuberLoss(delta=10)\n",
    "# def augment_switch(X,y):\n",
    "#     flips = torch.rand(y.shape[0]) < 0.5\n",
    "#     X[flips] = torch.stack([X[flips,:,:,1], X[flips,:,:,0]], dim=3)\n",
    "#     y[flips] = -y[flips]\n",
    "#     return X,y\n",
    "\n",
    "# def augment_shift(X,y, common_shift=True):\n",
    "#     X = torch.complex(X[:,0],X[:,1])\n",
    "\n",
    "#     #augment 1, multiply each of the vectors with phase\n",
    "\n",
    "#     imag_unit = torch.complex(torch.tensor(0.0),torch.tensor(1.0))\n",
    "#     if common_shift:\n",
    "#         sample_shift = (torch.rand(X.shape[0],1,1)*2 - 1)*config[\"max_shift\"]\n",
    "#         phase_shift = (-sample_shift/config[\"sample_length\"]*2*torch.pi*imag_unit*torch.arange(X.shape[1]).unsqueeze(1)).exp()\n",
    "#         new_abf = X*phase_shift\n",
    "#         X = torch.stack([torch.real(new_abf),torch.imag(new_abf)], dim=1)\n",
    "        \n",
    "#     else:\n",
    "#         sample_shift = (torch.rand(X.shape[0],1,2)*2 - 1)*config[\"max_shift\"]\n",
    "#         phase_shift = (-sample_shift/config[\"sample_length\"]*2*torch.pi*imag_unit*torch.arange(X.shape[1]).unsqueeze(1)).exp()\n",
    "#         new_abf = X*phase_shift\n",
    "#         X = torch.stack([torch.real(new_abf),torch.imag(new_abf)], dim=1)\n",
    "#         y = y + (sample_shift[:,:,0] - sample_shift[:,:,1]).to(int)\n",
    "#     return (X,y)\n",
    "\n",
    "# def augment_amp(X,y):\n",
    "#     amp_max = 3\n",
    "#     amp_min = 0.2\n",
    "#     amp_change_factor = torch.rand(X.shape)*(amp_max - amp_min) + amp_min\n",
    "#     X = X*amp_change_factor\n",
    "#     return (X,y)\n",
    "\n",
    "\n",
    "\n",
    "def format_simulated_data(X,y):\n",
    "    \"\"\"\n",
    "    transform a tensor of impulse responses in different rooms into pairwise TimeEstimation-problems. Note (X and y should be on GPU)\n",
    "\n",
    "    \"\"\"\n",
    "    #pull a random sound\n",
    "    sound_paths = glob(\"../datasets/reference_sounds/*.wav\")\n",
    "    sound_path = sound_paths[np.random.randint(len(sound_paths))]\n",
    "    fs,signal = wavfile.read(sound_path)\n",
    "    start = np.random.randint(0,len(signal) - config[\"sample_length\"] - config[\"rir_len\"]-1, config[\"rooms_per_batch\"])\n",
    "    # simulate longer sound and then cut to the relevant piece\n",
    "    signals = np.zeros((config[\"rooms_per_batch\"], config[\"sample_length\"] + config[\"rir_len\"]-1))\n",
    "    for i in range(config[\"rooms_per_batch\"]):\n",
    "        signals[i,:] = signal[start[i]:start[i]  + config[\"sample_length\"] + config[\"rir_len\"]-1]\n",
    "    signals = torch.tensor(signals).to(torch.float32).to(device).unsqueeze(1)\n",
    "\n",
    "    q = torch.fft.irfft(torch.fft.rfft(signals)*torch.fft.rfft(X,signals.shape[1]))[:,:,:config[\"sample_length\"]] # compute the heard sound, and cut it to the right length\n",
    "    q = torch.fft.rfft(q)[:,:,:config[\"max_freq\"]] # cut frequencies which are too high\n",
    "    q = q.unsqueeze(2)\n",
    "    q = torch.concatenate([torch.concatenate([q,q.roll(i+1, 1)], dim=2) for i in range(config[\"mics_per_batch\"] // 2)],dim=1) # organize sounds pairwise\n",
    "    q = q.view(config[\"rooms_per_batch\"]*(config[\"mics_per_batch\"]*(config[\"mics_per_batch\"] - 1 ))//2, 2,-1) # reshape so that each example is a row\n",
    "    X = torch.concatenate([q.real,q.imag],dim=1)\n",
    "    y = torch.concatenate([y - y.roll(i+1,1) for i in range(config[\"mics_per_batch\"]//2)],dim=1).view(-1)*fs/343 # compute gt for all pairs\n",
    "    y = y_to_class_gt(y, config[\"guess_grid_size\"], config[\"max_shift\"]).to(torch.long)\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, scheduler, warm_up=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    print_loss = 0\n",
    "    temp = 0\n",
    "    \n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        # Compute prediction error\n",
    "        batch_rooms = X.shape[0]\n",
    "        X = X.to(device)\n",
    "        \n",
    "        y = y.to(device)\n",
    "        X,y = format_simulated_data(X,y)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #increase learning rate if in wamup\n",
    "        if warm_up and batch%(1 + (len(dataloader)//steps_per_epoch)) == 0:\n",
    "            scheduler.step()\n",
    "            temp += 1\n",
    "            print(temp)\n",
    "\n",
    "        #printing and logging\n",
    "        print_loss += loss.detach()/config[\"n_batch_before_print\"]\n",
    "        if batch % config[\"n_batch_before_print\"] == 0:\n",
    "            if batch == 0:\n",
    "                print_loss = 0 \n",
    "                continue\n",
    "            if use_wandb:\n",
    "                wandb.log({\"loss\":loss})\n",
    "            loss, current = print_loss.item(), batch * batch_rooms\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            print_loss = 0\n",
    "     \n",
    "\n",
    "    \n",
    "def test(dataloader, model, loss_fn):\n",
    "    model.eval() # regression on dropout is not great\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print_loss = 0\n",
    "        counter = 0\n",
    "        for batch, (X,y) in enumerate(dataloader):\n",
    "            \n",
    "            # Compute prediction error\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            X,y = format_simulated_data(X,y)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "                \n",
    "            print_loss += loss.detach()\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "        loss = print_loss.item()/counter\n",
    "        if use_wandb:\n",
    "            wandb.log({\"Test_loss\":loss})\n",
    "        print(f\"Test loss: {loss:>7f}\")\n",
    "        print_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "schedule_step_counter = 0\n",
    "steps_per_epoch = 5\n",
    "warm_up_epochs = 2\n",
    "schedule_steps = warm_up_epochs*steps_per_epoch\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"]/2**schedule_steps)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test loss: 3.983006\n",
      "1\n",
      "loss: 3.980361  [   50/ 1800]\n",
      "loss: 3.999438  [  100/ 1800]\n",
      "loss: 4.133923  [  150/ 1800]\n",
      "loss: 3.902521  [  200/ 1800]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_73878/2355578574.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mwarm_up_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_73878/1516923196.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer, scheduler, warm_up)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_wandb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_rooms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mprint_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(config[\"epochs\"]):\n",
    "    losses = np.zeros((0,2))\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "    test(test_dl, model, loss_fn)\n",
    "    \n",
    "    train(train_dl, model, loss_fn, optimizer,scheduler, warm_up=(t<warm_up_epochs))\n",
    "    \n",
    "    if t % 10 == 0:\n",
    "        torch.save(model, \"./models/\" + config[\"dataset\"].split(\".\")[0]+ \"_\" + run.name + \".pth\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 9.765625e-08\n",
       "    lr: 0.0002\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iter(train_dl))//5\n",
    "optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
