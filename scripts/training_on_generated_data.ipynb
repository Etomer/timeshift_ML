{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import glob\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from bisect import bisect_left\n",
    "import scipy as sp\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../datasets/generated_dataset/\"\n",
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "dropout = 0.1\n",
    "batch_size = 32\n",
    "fs = 16000\n",
    "\n",
    "\n",
    "f = h5py.File(\"../datasets/generated_dataset/generated_dataset.hdf5\",\"r\")\n",
    "f_test = h5py.File(\"../datasets/generated_dataset/generated_dataset_val.hdf5\",\"r\")\n",
    "\n",
    "X = f['input']\n",
    "y = f['gt']\n",
    "X_test = f_test['input']\n",
    "y_test = f_test['gt']\n",
    "\n",
    "#X = torch.load(os.path.join(data_folder,\"input2.pt\"))\n",
    "#y = torch.load(os.path.join(data_folder,\"gt2.pt\"))\n",
    "#X_test = torch.load(os.path.join(data_folder,\"input.pt\"))\n",
    "#y_test = torch.load(os.path.join(data_folder,\"gt.pt\"))\n",
    "\n",
    "class custom_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx],self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "dataset = custom_dataset(X,y)\n",
    "dataset_test = custom_dataset(X_test,y_test)\n",
    "train_dl = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 2\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,size):\n",
    "        super().__init__()\n",
    "        self.l = nn.Linear(size,size)\n",
    "        self.act = nn.GELU()\n",
    "        #self.ln = nn.LayerNorm(size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.act(self.l(x))\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.ln1 = nn.LayerNorm(6250)\n",
    "        #self.ln2 = nn.LayerNorm(2016)\n",
    "        \n",
    "        self.thinker = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            Block(2016*factor),\n",
    "            Block(2016*factor),\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(4,16*factor, 50,stride=5),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(16*factor,32*factor, 50,stride=5),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(32*factor,48*factor, 30,stride=5),\n",
    "            nn.GELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.compress = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2016*factor, 10*factor),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(10*factor, 1),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        \n",
    "        x = self.thinker(x)\n",
    "        return self.compress(x)\n",
    "    \n",
    "model = Classifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "#loss_fn = torch.nn.HuberLoss(delta=10)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    save_loss_to_file = []\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    n_batch_before_print = 200\n",
    "    print_loss = 0\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        \n",
    "        # Compute prediction error\n",
    "        X = torch.concatenate([X[:,:,:,0], X[:,:,:,1]], dim=1)\n",
    "        X /= 1e6\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        \n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print_loss += loss.detach()/n_batch_before_print\n",
    "        if batch % n_batch_before_print == 0 and batch != 0:\n",
    "            loss, current = print_loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            print_loss = 0\n",
    "            save_loss_to_file.append((loss, current))\n",
    "    return save_loss_to_file\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.train()\n",
    "        print_loss = 0\n",
    "        for batch, (X,y) in enumerate(dataloader):\n",
    "            \n",
    "            # Compute prediction error\n",
    "            X = torch.concatenate([X[:,:,:,0], X[:,:,:,1]], dim=1)\n",
    "            X /= 1e6\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            \n",
    "\n",
    "            loss = loss_fn(pred, y)        \n",
    "            print_loss += loss.detach()\n",
    "\n",
    "        loss = print_loss.item()/len(dataloader)\n",
    "        print(f\"Test loss: {loss:>7f}\")\n",
    "        print_loss = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test loss: 14813.876562\n",
      "loss: 11162.006836  [ 6400/125000]\n",
      "loss: 11244.174805  [12800/125000]\n",
      "loss: 11683.435547  [19200/125000]\n",
      "loss: 8979.286133  [25600/125000]\n",
      "loss: 7260.247559  [32000/125000]\n",
      "loss: 6710.510254  [38400/125000]\n",
      "loss: 6907.070312  [44800/125000]\n",
      "loss: 6454.831543  [51200/125000]\n",
      "loss: 6347.410645  [57600/125000]\n",
      "loss: 6074.241699  [64000/125000]\n",
      "loss: 6222.403809  [70400/125000]\n",
      "loss: 5759.708496  [76800/125000]\n",
      "loss: 6035.328125  [83200/125000]\n",
      "loss: 5989.769043  [89600/125000]\n",
      "loss: 5752.730469  [96000/125000]\n",
      "loss: 5823.321777  [102400/125000]\n",
      "loss: 5737.701172  [108800/125000]\n",
      "loss: 5498.667969  [115200/125000]\n",
      "loss: 5543.495117  [121600/125000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test loss: 7675.120313\n",
      "loss: 5480.889648  [ 6400/125000]\n",
      "loss: 5345.096680  [12800/125000]\n",
      "loss: 5311.378906  [19200/125000]\n",
      "loss: 5286.786133  [25600/125000]\n",
      "loss: 5349.657227  [32000/125000]\n",
      "loss: 5185.663574  [38400/125000]\n",
      "loss: 5313.774414  [44800/125000]\n",
      "loss: 5145.198242  [51200/125000]\n",
      "loss: 5348.059570  [57600/125000]\n",
      "loss: 5176.958496  [64000/125000]\n",
      "loss: 5338.368652  [70400/125000]\n",
      "loss: 5202.345215  [76800/125000]\n",
      "loss: 5131.703125  [83200/125000]\n",
      "loss: 5274.237793  [89600/125000]\n",
      "loss: 5220.040527  [96000/125000]\n",
      "loss: 5134.492188  [102400/125000]\n",
      "loss: 5056.882324  [108800/125000]\n",
      "loss: 4976.488281  [115200/125000]\n",
      "loss: 5012.821289  [121600/125000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test loss: 6890.996875\n",
      "loss: 4692.439941  [ 6400/125000]\n",
      "loss: 4960.300293  [12800/125000]\n",
      "loss: 4996.293945  [19200/125000]\n",
      "loss: 4837.872070  [25600/125000]\n",
      "loss: 4970.949707  [32000/125000]\n",
      "loss: 5019.455078  [38400/125000]\n",
      "loss: 4923.622070  [44800/125000]\n",
      "loss: 4992.565918  [51200/125000]\n",
      "loss: 4756.875000  [57600/125000]\n",
      "loss: 4836.360352  [64000/125000]\n",
      "loss: 4711.476562  [70400/125000]\n",
      "loss: 4975.212891  [76800/125000]\n",
      "loss: 4837.316406  [83200/125000]\n",
      "loss: 4929.734375  [89600/125000]\n",
      "loss: 4776.627441  [96000/125000]\n",
      "loss: 4706.235352  [102400/125000]\n",
      "loss: 4747.930664  [108800/125000]\n",
      "loss: 4761.407715  [115200/125000]\n",
      "loss: 4856.329590  [121600/125000]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test loss: 7187.887500\n",
      "loss: 4584.436035  [ 6400/125000]\n",
      "loss: 4571.973633  [12800/125000]\n",
      "loss: 4480.646484  [19200/125000]\n",
      "loss: 4769.050293  [25600/125000]\n",
      "loss: 4638.937988  [32000/125000]\n",
      "loss: 4584.974121  [38400/125000]\n",
      "loss: 4638.354980  [44800/125000]\n",
      "loss: 4658.672852  [51200/125000]\n",
      "loss: 4734.932617  [57600/125000]\n",
      "loss: 4666.605469  [64000/125000]\n",
      "loss: 4548.348145  [70400/125000]\n",
      "loss: 4545.904785  [76800/125000]\n",
      "loss: 4518.637207  [83200/125000]\n",
      "loss: 4586.571777  [89600/125000]\n",
      "loss: 4495.704102  [96000/125000]\n",
      "loss: 4495.749023  [102400/125000]\n",
      "loss: 4679.332520  [108800/125000]\n",
      "loss: 4428.327637  [115200/125000]\n",
      "loss: 4407.532227  [121600/125000]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test loss: 7465.838281\n",
      "loss: 4530.359863  [ 6400/125000]\n",
      "loss: 4306.028320  [12800/125000]\n",
      "loss: 4547.008301  [19200/125000]\n",
      "loss: 4166.087891  [25600/125000]\n",
      "loss: 4268.439941  [32000/125000]\n",
      "loss: 4361.950195  [38400/125000]\n",
      "loss: 4384.441895  [44800/125000]\n",
      "loss: 4404.386719  [51200/125000]\n",
      "loss: 4470.375488  [57600/125000]\n",
      "loss: 4457.070801  [64000/125000]\n",
      "loss: 4386.621094  [70400/125000]\n",
      "loss: 4215.686035  [76800/125000]\n",
      "loss: 4302.971680  [83200/125000]\n",
      "loss: 4427.558105  [89600/125000]\n",
      "loss: 4454.345215  [96000/125000]\n",
      "loss: 4465.001465  [102400/125000]\n",
      "loss: 4285.850586  [108800/125000]\n",
      "loss: 4359.528320  [115200/125000]\n",
      "loss: 4184.404785  [121600/125000]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test loss: 6524.760938\n",
      "loss: 4193.514160  [ 6400/125000]\n",
      "loss: 3901.470703  [12800/125000]\n",
      "loss: 4132.829590  [19200/125000]\n",
      "loss: 4091.165771  [25600/125000]\n",
      "loss: 4112.897949  [32000/125000]\n",
      "loss: 4362.731445  [38400/125000]\n",
      "loss: 3985.668213  [44800/125000]\n",
      "loss: 4136.415527  [51200/125000]\n",
      "loss: 4102.476562  [57600/125000]\n",
      "loss: 4266.844238  [64000/125000]\n",
      "loss: 4275.523926  [70400/125000]\n",
      "loss: 4022.276611  [76800/125000]\n",
      "loss: 4254.462891  [83200/125000]\n",
      "loss: 4264.110352  [89600/125000]\n",
      "loss: 4118.505371  [96000/125000]\n",
      "loss: 4007.760254  [102400/125000]\n",
      "loss: 4247.025879  [108800/125000]\n",
      "loss: 4052.663574  [115200/125000]\n",
      "loss: 4206.945801  [121600/125000]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test loss: 6280.099219\n",
      "loss: 3964.541260  [ 6400/125000]\n",
      "loss: 4000.884521  [12800/125000]\n",
      "loss: 3954.756592  [19200/125000]\n",
      "loss: 3893.916016  [25600/125000]\n",
      "loss: 3829.849609  [32000/125000]\n",
      "loss: 4053.343994  [38400/125000]\n",
      "loss: 4141.224609  [44800/125000]\n",
      "loss: 3955.265381  [51200/125000]\n",
      "loss: 3894.104004  [57600/125000]\n",
      "loss: 4002.654541  [64000/125000]\n",
      "loss: 3986.264893  [70400/125000]\n",
      "loss: 3930.059814  [76800/125000]\n",
      "loss: 3981.626953  [83200/125000]\n",
      "loss: 3962.312500  [89600/125000]\n",
      "loss: 3939.614014  [96000/125000]\n",
      "loss: 4174.283203  [102400/125000]\n",
      "loss: 3934.579834  [108800/125000]\n",
      "loss: 3965.608887  [115200/125000]\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "schedule_steps = 0\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=10)\n",
    "for t in range(epochs):\n",
    "    losses = np.zeros((0,2))\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    #torch.save(model, \"model_musan_epoch_\" + str(t)+ \".pth\")\n",
    "\n",
    "    loss_to_file = test(test_dl, model, loss_fn)\n",
    "    loss_to_file = train(train_dl, model, loss_fn, optimizer)\n",
    " #   if t < schedule_steps:\n",
    " #       scheduler.step()\n",
    "    \n",
    "    #losses = np.concatenate([losses,np.array(loss_to_file\n",
    "    # )])\n",
    "    #np.save(\"losses\", losses)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2d5f891c0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dataset[0][0][1,:,0])\n",
    "#next(iter(train_dl))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
