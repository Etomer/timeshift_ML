{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import glob\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from bisect import bisect_left\n",
    "import scipy as sp\n",
    "import h5py\n",
    "from scipy.io import wavfile\n",
    "from glob import glob\n",
    "import sys\n",
    "sys.path.append(\"../models/\")\n",
    "import gcc_phat\n",
    "\n",
    "config = {\n",
    "    #\"continue_training_from_checkpoint\" : \"./models/new_type_dataset_medium_divine-shadow-51_190.pth\",\n",
    "    \"dropout\" : 0.1,\n",
    "    \"batch_size\" : 30, \n",
    "    \"max_shift\" : 500, # ~10 meters\n",
    "    \"guess_grid_size\" : 100,\n",
    "    \"dataset\" : \"new_type_dataset_medium.hdf5\",\n",
    "    \"cnn_output_size_at_factor_1\" : 576,\n",
    "    \"factor\" : 10,\n",
    "    \"loss_fn\" : \"cross_entropy\",\n",
    "    \"epochs\" : 1000,\n",
    "    \"sample_length\" : 10000,\n",
    "    \"lr\" : 3e-5,\n",
    "    \"n_batch_before_print\" : 3,\n",
    "    \"max_freq\" : 2500,\n",
    "    \"rir_len\" : 1600,\n",
    "    \"rooms_per_batch\" : 30,\n",
    "    \"mics_per_batch\" : 15,\n",
    "    \"warmup_steps_per_epoch\" : 5,\n",
    "    \"warmup_epochs\" : 2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_len = 10000\n",
    "max_shift = 500\n",
    "guess_grid_size = 100\n",
    "fs = 16000\n",
    "\n",
    "dataset = (\"../data/datasets/paired_fft_music_0014_evaluation.hdf5\")\n",
    "#dataset = (\"../data/datasets/impulse_response_easy.hdf5\")\n",
    "\n",
    "model = gcc_phat.gcc_phat_model(sample_len, max_shift,guess_grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6780, 4, 2500)\n",
      "inlier ratio : 0.54\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQlUlEQVR4nO3df6zddX3H8edrreLUGEt6y2pbd+tSncW5aK6MjWxhIqNTQvljJCXBNJOlcUGHi0ZbScZfTchc1CWOJY0wukggjeJoNDpr1ZElA3YBUUrFNpJBpdLryKaZSV3xvT/uF3e4nMu99/zobT88Hwk55/v5fr7n+/5cel7n00/P93tTVUiS2vIry12AJGn0DHdJapDhLkkNMtwlqUGGuyQ1aOVyFwCwevXqmpycXO4yJOms8sADD/y4qib67Tsjwn1ycpLp6enlLkOSzipJ/mO+fS7LSFKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgxYM9yS3JjmR5JE57R9M8liSQ0n+uqd9V5Kj3b7LxlG0dDpM7vzycpcgDWwxtx+4DfgM8I/PNST5Q2Ar8NaqOplkTde+GdgGnA+8Dvh6kjdW1bOjLlySNL8FZ+5VdQ/wzJzmPwduqqqTXZ8TXftW4M6qOllVjwNHgQtGWK8kaREGXXN/I/D7Se5L8i9J3tG1rwOe7Ol3rGuTJJ1Gg94VciWwCrgQeAewL8kbgPTp2/c3cCfZAewAeP3rXz9gGZKkfgaduR8D7qpZ9wO/AFZ37Rt6+q0Hnur3AlW1p6qmqmpqYqLv7YglSQMaNNz/CXgnQJI3Ai8HfgzsB7YlOSfJRmATcP8I6pQkLcGCyzJJ7gAuBlYnOQbcCNwK3Np9PfLnwPaqKuBQkn3Ao8Ap4Dq/KSNJp9+C4V5VV8+z65p5+u8Gdg9TlCRpOF6hKkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ1aMNyT3JrkRPcr9ebu+0iSSrK6p21XkqNJHkty2agLliQtbDEz99uALXMbk2wALgWe6GnbDGwDzu+OuTnJipFUKklatAXDvaruAZ7ps+tTwEeB6mnbCtxZVSer6nHgKHDBKAqVJC3eQGvuSa4AflhVD8/ZtQ54smf7WNfW7zV2JJlOMj0zMzNIGZKkeSw53JO8ErgB+Kt+u/u0VZ82qmpPVU1V1dTExMRSy5AkvYiVAxzzG8BG4OEkAOuBB5NcwOxMfUNP3/XAU8MWKUlamiXP3Kvqu1W1pqomq2qS2UB/e1X9CNgPbEtyTpKNwCbg/pFWLEla0GK+CnkH8G/Am5IcS3LtfH2r6hCwD3gU+CpwXVU9O6piJUmLs+CyTFVdvcD+yTnbu4Hdw5UlSRqGV6hKUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhq0mN/EdGuSE0ke6Wn7RJLvJflOki8meW3Pvl1JjiZ5LMllY6pbkvQiFjNzvw3YMqftAPCWqnor8H1gF0CSzcA24PzumJuTrBhZtZKkRVkw3KvqHuCZOW1fq6pT3ea9wPru+Vbgzqo6WVWPA0eBC0ZYryRpEUax5v4+4Cvd83XAkz37jnVtL5BkR5LpJNMzMzMjKEOS9Jyhwj3JDcAp4Pbnmvp0q37HVtWeqpqqqqmJiYlhypAkzbFy0AOTbAcuBy6pqucC/BiwoafbeuCpwcuTJA1ioJl7ki3Ax4ArqupnPbv2A9uSnJNkI7AJuH/4MiVJS7HgzD3JHcDFwOokx4Abmf12zDnAgSQA91bV+6vqUJJ9wKPMLtdcV1XPjqt4SVJ/C4Z7VV3dp/mWF+m/G9g9TFGSpOF4haokNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1aMFwT3JrkhNJHulpOzfJgSRHusdVPft2JTma5LEkl42rcEnS/BYzc78N2DKnbSdwsKo2AQe7bZJsBrYB53fH3JxkxciqlSQtyoLhXlX3AM/Mad4K7O2e7wWu7Gm/s6pOVtXjwFHggtGUKklarEHX3M+rquMA3eOarn0d8GRPv2Nd2wsk2ZFkOsn0zMzMgGVIkvoZ9T+opk9b9etYVXuqaqqqpiYmJkZchiS9tA0a7k8nWQvQPZ7o2o8BG3r6rQeeGrw8SdIgBg33/cD27vl24O6e9m1JzkmyEdgE3D9ciZKkpVq5UIckdwAXA6uTHANuBG4C9iW5FngCuAqgqg4l2Qc8CpwCrquqZ8dUuyRpHguGe1VdPc+uS+bpvxvYPUxRkqTheIWqJDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEuLMLnzy8tdgrQkhrskNchwl6QGGe6S1KChwj3JXyY5lOSRJHckeUWSc5McSHKke1w1qmIlSYszcLgnWQf8BTBVVW8BVgDbgJ3AwaraBBzstiVJp9GwyzIrgV9NshJ4JfAUsBXY2+3fC1w55DkkSUs0cLhX1Q+Bv2H2F2QfB/67qr4GnFdVx7s+x4E1/Y5PsiPJdJLpmZmZQcuQJPUxzLLMKmZn6RuB1wGvSnLNYo+vqj1VNVVVUxMTE4OWIUnqY5hlmXcBj1fVTFX9L3AX8HvA00nWAnSPJ4YvU5K0FMOE+xPAhUlemSTAJcBhYD+wveuzHbh7uBIlSUu1ctADq+q+JJ8HHgROAQ8Be4BXA/uSXMvsB8BVoyhUkrR4A4c7QFXdCNw4p/kks7N4SdIy8QpVSWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDhgr3JK9N8vkk30tyOMnvJjk3yYEkR7rHVaMqVpK0OMPO3P8W+GpV/Sbw28z+DtWdwMGq2gQc7LYlSafRwOGe5DXAHwC3AFTVz6vqv4CtwN6u217gyuFKlCQt1TAz9zcAM8A/JHkoyWeTvAo4r6qOA3SPa0ZQpyRpCYYJ95XA24G/r6q3Af/DEpZgkuxIMp1kemZmZogyJElzDRPux4BjVXVft/15ZsP+6SRrAbrHE/0Orqo9VTVVVVMTExNDlCFJmmvgcK+qHwFPJnlT13QJ8CiwH9jetW0H7h6qQknSkq0c8vgPArcneTnwA+BPmf3A2JfkWuAJ4KohzyFJWqKhwr2qvg1M9dl1yTCvK0kajleoSlKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoOGDvckK5I8lORL3fa5SQ4kOdI9rhq+TEnSUoxi5n49cLhneydwsKo2AQe7bUnSaTRUuCdZD7wH+GxP81Zgb/d8L3DlMOeQJC3dsDP3TwMfBX7R03ZeVR0H6B7X9DswyY4k00mmZ2ZmhixDktRr4HBPcjlwoqoeGOT4qtpTVVNVNTUxMTFoGZKkPlYOcexFwBVJ3g28AnhNks8BTydZW1XHk6wFToyiUEnS4g08c6+qXVW1vqomgW3AN6rqGmA/sL3rth24e+gqJUlLMo7vud8EXJrkCHBpty1JOo2GWZb5par6FvCt7vl/ApeM4nUlSYPxClVJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe7SCEzu/PJylyA9j+Eu9TCk1QrDXZIaNMwvyN6Q5JtJDic5lOT6rv3cJAeSHOkeV42uXEnSYgwzcz8FfLiq3gxcCFyXZDOwEzhYVZuAg922JOk0GuYXZB+vqge75z8FDgPrgK3A3q7bXuDKIWuUziiuy+tsMJI19ySTwNuA+4Dzquo4zH4AAGvmOWZHkukk0zMzM6MoQ5LUGTrck7wa+ALwoar6yWKPq6o9VTVVVVMTExPDliFJ6jFUuCd5GbPBfntV3dU1P51kbbd/LXBiuBIlSUs1zLdlAtwCHK6qT/bs2g9s755vB+4evDzp7OOavM4Ew8zcLwLeC7wzybe7/94N3ARcmuQIcGm3LTXJINeZauWgB1bVvwKZZ/clg76uJGl4XqEqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdpjLzIScvFcJekBhnuktQgw12SGmS46yXvdK2Lu/6u08lwlwT44dMaw12SGmS4S1KDDHdpGSxlCWTQvuNcZhlX/Rodw/0sMMyb47ljF/Mag55nXG/el3KAnAk/U53dxhbuSbYkeSzJ0SQ7x3We5dDvDXK63zRn6pt02A+TpR631J/D6ZrZnmlGMdZhftajeL3lOu5sNZZwT7IC+Dvgj4HNwNVJNo/jXJKkFxrXzP0C4GhV/aCqfg7cCWwd07nGNkMY1FLqma/vcq+X9puBL8fsbxTHnckztqX+fEf9/2Uxf/5G8ed5Kedb7GvN13/Qn+lyGOe5U1Wjf9HkT4AtVfVn3fZ7gd+pqg/09NkB7Og23wQ8NvJCzgyrgR8vdxFj5hjb4BjPPr9eVRP9dqwc0wnTp+15nyJVtQfYM6bznzGSTFfV1HLXMU6OsQ2OsS3jWpY5Bmzo2V4PPDWmc0mS5hhXuP87sCnJxiQvB7YB+8d0LknSHGNZlqmqU0k+APwzsAK4taoOjeNcZ4Hml55wjK1wjA0Zyz+oSpKWl1eoSlKDDHdJapDhPmZJPpKkkqzuadvV3ZbhsSSXLWd9w0jyiSTfS/KdJF9M8tqefU2MEdq7lUaSDUm+meRwkkNJru/az01yIMmR7nHVctc6rCQrkjyU5EvddnNjnI/hPkZJNgCXAk/0tG1m9ttD5wNbgJu72zWcjQ4Ab6mqtwLfB3ZBW2Ns9FYap4APV9WbgQuB67ox7QQOVtUm4GC3fba7Hjjcs93iGPsy3MfrU8BHef4FXFuBO6vqZFU9Dhxl9nYNZ52q+lpVneo272X2egZoaIyc5ltpnA5VdbyqHuye/5TZ8FvH7Lj2dt32AlcuS4EjkmQ98B7gsz3NTY3xxRjuY5LkCuCHVfXwnF3rgCd7to91bWe79wFf6Z63NMaWxvICSSaBtwH3AedV1XGY/QAA1ixjaaPwaWYnV7/oaWttjPMa1+0HXhKSfB34tT67bgA+DvxRv8P6tJ2x30d9sTFW1d1dnxuY/av+7c8d1qf/GTvGBbQ0ludJ8mrgC8CHquonSb+hnp2SXA6cqKoHkly8zOUsC8N9CFX1rn7tSX4L2Ag83L1h1gMPJrmAs+zWDPON8TlJtgOXA5fU/180cVaNcQEtjeWXkryM2WC/varu6pqfTrK2qo4nWQucWL4Kh3YRcEWSdwOvAF6T5HO0NcYX5bLMGFTVd6tqTVVNVtUkswHx9qr6EbO3YdiW5JwkG4FNwP3LWO7AkmwBPgZcUVU/69nVzBhp8FYamZ1x3AIcrqpP9uzaD2zvnm8H7j7dtY1KVe2qqvXd+28b8I2quoaGxrgQZ+6nWVUdSrIPeJTZpYzrqurZZS5rUJ8BzgEOdH9Dubeq3t/SGBu9lcZFwHuB7yb5dtf2ceAmYF+Sa5n9htdVy1PeWL0Uxgh4+wFJapLLMpLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNej/AEyvwpRfwGSqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def y_to_class_gt(y, guess_grid_size, max_shift) :\n",
    "        y[y.abs() > max_shift] = max_shift*y[y.abs() > max_shift].sign()\n",
    "\n",
    "        bin_width = max_shift*2/guess_grid_size\n",
    "        y = (y/bin_width).round() + guess_grid_size // 2 \n",
    "        y[y == guess_grid_size] = guess_grid_size - 1\n",
    "        return y.long()\n",
    "\n",
    "def format_simulated_data(X,y):\n",
    "    \"\"\"\n",
    "    transform a tensor of impulse responses in different rooms into pairwise TimeEstimation-problems. Note (X and y should be on GPU)\n",
    "\n",
    "    \"\"\"\n",
    "    #pull a random sound\n",
    "    sound_paths = glob(\"../data/reference_data/reference_sounds/*.wav\")\n",
    "    sound_path = sound_paths[np.random.randint(len(sound_paths))]\n",
    "    fs,signal = wavfile.read(sound_path)\n",
    "    start = np.random.randint(0,len(signal) - config[\"sample_length\"] - config[\"rir_len\"]-1, X.shape[0])\n",
    "    # simulate longer sound and then cut to the relevant piece\n",
    "    signals = np.zeros((X.shape[0], config[\"sample_length\"] + config[\"rir_len\"]-1))\n",
    "    for i in range(X.shape[0]):\n",
    "        signals[i,:] = signal[start[i]:start[i]  + config[\"sample_length\"] + config[\"rir_len\"]-1]\n",
    "    #signals = torch.tensor(signals).to(torch.float32).to(device).unsqueeze(1)\n",
    "    signals = torch.tensor(signals).to(torch.float32).unsqueeze(1)\n",
    "    q = torch.fft.irfft(torch.fft.rfft(signals)*torch.fft.rfft(X,signals.shape[2]))[:,:,:config[\"sample_length\"]] # compute the heard sound, and cut it to the right length\n",
    "    q = torch.fft.rfft(q)[:,:,:config[\"max_freq\"]] # cut frequencies which are too high\n",
    "    q = q.unsqueeze(2)\n",
    "    q = torch.concatenate([torch.concatenate([q,q.roll(i+1, 1)], dim=2) for i in range(config[\"mics_per_batch\"] // 2)],dim=1) # organize sounds pairwise\n",
    "    q = q.view(X.shape[0]*(config[\"mics_per_batch\"]*(config[\"mics_per_batch\"] - 1 ))//2, 2,-1) # reshape so that each example is a row\n",
    "    X = torch.concatenate([q.real,q.imag],dim=1)\n",
    "    y = torch.concatenate([y - y.roll(i+1,1) for i in range(config[\"mics_per_batch\"]//2)],dim=1).view(-1)*fs/343 # compute gt for all pairs\n",
    "    y = y_to_class_gt(y, config[\"guess_grid_size\"], config[\"max_shift\"]).to(torch.long)\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "with h5py.File(dataset, 'r') as hdf5_file:\n",
    "    X_all = hdf5_file[\"input\"] \n",
    "    y_all = hdf5_file[\"gt\"] \n",
    "    print(X_all.shape)\n",
    "\n",
    "    first = 3000\n",
    "    indx = torch.arange(first,first + 500)\n",
    "    \n",
    "    # X = torch.tensor(X_all[indx, :15])\n",
    "    # y = torch.tensor(y_all[indx, :15])\n",
    "    # X,y = format_simulated_data(X,y)\n",
    "\n",
    "    X = torch.tensor(X_all[indx])\n",
    "    y = torch.tensor(y_all[indx])\n",
    "    y = y_to_class_gt(y*fs/343, config[\"guess_grid_size\"], config[\"max_shift\"]).to(torch.long)\n",
    "    #print(y)\n",
    "\n",
    "    pred = model(X).argmax(dim=1)\n",
    "    plt.hist(pred - y,300)\n",
    "\n",
    "    correct = ((pred - y).abs() < 2).sum()\n",
    "    print(f'inlier ratio : {(correct/y.numel()).item():.2f}')\n",
    "    #plt.plot(pred[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,size):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "        self.l = nn.Linear(size,2*size)\n",
    "        self.l2 = nn.Linear(2*size,size)\n",
    "        self.act = nn.GELU()\n",
    "        self.ln = nn.LayerNorm(size)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.002)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.002)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return x + self.l2(self.act(self.l(self.ln(self.dropout(x)))))\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.thinker = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(config[\"dropout\"]),\n",
    "            nn.Linear(config[\"factor\"]*config[\"cnn_output_size_at_factor_1\"],1000),\n",
    "            nn.GELU(),\n",
    "            #Block(1000),\n",
    "            #Block(1000),\n",
    "            nn.Linear(1000,config[\"guess_grid_size\"])\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(4,48*config[\"factor\"], 50,stride=5),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(48*config[\"factor\"],48*config[\"factor\"], 50,stride=5),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(48*config[\"factor\"],48*config[\"factor\"], 30,stride=5),\n",
    "            nn.GELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.00002)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.00002)\n",
    "        \n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.thinker(x)\n",
    "        return x\n",
    "    \n",
    "model = Classifier().to(device)\n",
    "model = torch.load(\"../models/new_type_dataset_medium_sage-shadow-58_100.pth\",map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets_folder = \"../data/datasets/\"\n",
    "# dataset = \"impulse_response_hard_evaluation.hdf5\"\n",
    "\n",
    "\n",
    "# with h5py.File(datasets_folder + dataset, 'a') as h5_file:\n",
    "\n",
    "#     X = h5_file[\"input\"]\n",
    "#     X.attrs.create(\"dataset_type\", \"impulse_response\")\n",
    "#     X.attrs.create(\"reflection_coeff\", 0.3)\n",
    "\n",
    "#     X.attrs.create(\"scatter_coeff\", 0.15)\n",
    "#     X.attrs.create(\"sample_length\", 10000)    \n",
    "#     X.attrs.create(\"fs\", 16000)\n",
    "#     X.attrs.create(\"speed_of_sound\", 343)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# datasets_folder = \"../data/datasets/\"\n",
    "# dataset = \"paired_fft_music_0014_evaluation.hdf5\"\n",
    "\n",
    "\n",
    "# with h5py.File(datasets_folder + dataset, 'a') as h5_file:\n",
    "\n",
    "#     X = h5_file[\"input\"]\n",
    "#     X.attrs.create(\"dataset_type\", \"paired_fft\")\n",
    "    \n",
    "#     X.attrs.create(\"sample_length\", 10000)    \n",
    "#     X.attrs.create(\"fs\", 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['dataset_type', 'fs', 'sample_length']>\n"
     ]
    }
   ],
   "source": [
    "# with h5py.File(datasets_folder + dataset, 'r') as h5_file:\n",
    "#     X = h5_file[\"input\"]\n",
    "#     print(X.attrs.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
